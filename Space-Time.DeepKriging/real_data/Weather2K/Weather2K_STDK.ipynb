{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a51d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n",
      "Weather2K shape: (1866, 13, 13632)\n",
      "Using last 100 timesteps | Y shape: (1866, 100)\n",
      "\n",
      "[Stage 3] Precompute bases ...\n",
      "Space basis dim: 238\n",
      "Time  basis dim: 70\n",
      "Total embedding dim: 308\n",
      "[Rep 01/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 02/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 03/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 04/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 05/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 06/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 07/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 08/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 09/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "[Rep 10/10] obs=187 train=168 val=19 miss=1679 | Crossing rate = 0.000000\n",
      "\n",
      "=== Summary over repetitions (resample split each rep) ===\n",
      "Crossing rate mean = 0.000000\n",
      "Crossing rate SD   = 0.000000\n",
      "CRPS_z   mean = 0.232722 | SD = 0.021029\n",
      "CRPS_raw mean = 1.352524 | SD = 0.096596\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "# Global settings & dirs\n",
    "BASE_SEED = 2024\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(BASE_SEED)\n",
    "\n",
    "DATA_PATH = Path(\"/home/wangxc1117/Weather2K/weather2k.npy\")\n",
    "\n",
    "TARGET_IDX = 4\n",
    "OBS_RATIO = 0.10\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "\n",
    "GRID_SIZES = (5, 9, 12)\n",
    "H_LIST = (10, 15, 45)\n",
    "\n",
    "STATIONS_PER_CHUNK = 100\n",
    "T_KEEP = 100\n",
    "\n",
    "VAL_STATION_RATIO = 0.10\n",
    "PATIENCE = 30\n",
    "\n",
    "TAUS = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
    "\n",
    "N_REP = 10\n",
    "SEED_OFFSET = 1000\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"GPUs:\", gpus)\n",
    "\n",
    "\n",
    "def wendland_c2(d):\n",
    "    out = np.zeros_like(d, dtype=np.float32)\n",
    "    m = (d >= 0.0) & (d <= 1.0)\n",
    "    dm = d[m]\n",
    "    out[m] = ((1.0 - dm) ** 6) * (35.0 * dm**2 + 18.0 * dm + 3.0) / 3.0\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_space_basis(s_xy, grid_sizes=GRID_SIZES, theta_scale=2.5):\n",
    "    n = s_xy.shape[0]\n",
    "    cols = []\n",
    "    for g in grid_sizes:\n",
    "        knots_1d = np.linspace(0.0, 1.0, g, dtype=np.float32)\n",
    "        kx, ky = np.meshgrid(knots_1d, knots_1d)\n",
    "        knots = np.column_stack([kx.ravel(), ky.ravel()]).astype(np.float32)\n",
    "\n",
    "        spacing = 1.0 / (g - 1)\n",
    "        theta = theta_scale * spacing\n",
    "\n",
    "        phi = np.zeros((n, knots.shape[0]), dtype=np.float32)\n",
    "        for j in range(knots.shape[0]):\n",
    "            d = np.linalg.norm(s_xy - knots[j], axis=1) / (theta + 1e-12)\n",
    "            phi[:, j] = wendland_c2(d)\n",
    "\n",
    "        cols.append(phi)\n",
    "    return np.concatenate(cols, axis=1)\n",
    "\n",
    "\n",
    "def build_time_basis(t_norm, H_list=H_LIST):\n",
    "    cols = []\n",
    "    for H in H_list:\n",
    "        knots = np.linspace(0.0, 1.0, H, dtype=np.float32)\n",
    "        kappa = abs(knots[1] - knots[0]) if H >= 2 else 1.0\n",
    "        diff = (t_norm[:, None] - knots[None, :]) / (kappa + 1e-12)\n",
    "        cols.append(np.exp(-0.5 * diff**2).astype(np.float32))\n",
    "    return np.concatenate(cols, axis=1)\n",
    "\n",
    "\n",
    "def tilted_loss(tau):\n",
    "    tau = float(tau)\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return tf.reduce_mean(tf.maximum(tau * e, (tau - 1.0) * e))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model_multi_quantile(input_dim):\n",
    "    reg = regularizers.L1L2(l1=1e-5, l2=1e-4)\n",
    "\n",
    "    inp = keras.Input(shape=(input_dim,), name=\"X\")\n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=\"random_normal\", kernel_regularizer=reg)(inp)\n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=\"random_normal\", kernel_regularizer=reg)(x)\n",
    "\n",
    "    for _ in range(6):\n",
    "        x = Dense(100, activation=\"relu\", kernel_initializer=\"random_normal\")(x)\n",
    "\n",
    "    x = Dense(50, activation=\"relu\", kernel_initializer=\"random_normal\")(x)\n",
    "    x = Dense(50, activation=\"relu\", kernel_initializer=\"random_normal\")(x)\n",
    "\n",
    "    outputs = {}\n",
    "    losses = {}\n",
    "    for tau in TAUS:\n",
    "        name = f\"q{str(tau).replace('.','_')}\"\n",
    "        outputs[name] = Dense(1, kernel_initializer=\"random_normal\", name=name)(x)\n",
    "        losses[name] = tilted_loss(tau)\n",
    "\n",
    "    model = keras.Model(inputs=inp, outputs=outputs, name=\"STDK_MultiQuantile\")\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LR), loss=losses)\n",
    "    return model\n",
    "\n",
    "\n",
    "def crps_from_quantiles_weighted(y_true, preds_dict, taus=TAUS):\n",
    "    y = np.asarray(y_true).reshape(-1).astype(np.float64)\n",
    "\n",
    "    taus = np.asarray(taus, dtype=np.float64)\n",
    "    taus = np.sort(taus)\n",
    "\n",
    "    w = np.zeros_like(taus)\n",
    "    w[0] = 0.5 * (taus[1] - taus[0])\n",
    "    w[-1] = 0.5 * (taus[-1] - taus[-2])\n",
    "    w[1:-1] = 0.5 * (taus[2:] - taus[:-2])\n",
    "\n",
    "    check_sum = np.zeros_like(y, dtype=np.float64)\n",
    "    for tau, wk in zip(taus, w):\n",
    "        key = f\"q{str(tau).replace('.','_')}\"\n",
    "        q = np.asarray(preds_dict[key]).reshape(-1).astype(np.float64)\n",
    "        e = y - q\n",
    "        check = np.maximum(tau * e, (tau - 1.0) * e)\n",
    "        check_sum += wk * check\n",
    "\n",
    "    return 2.0 * check_sum\n",
    "\n",
    "\n",
    "def crossing_rate_from_preds(preds_dict, taus=TAUS):\n",
    "    taus = np.asarray(sorted(taus), dtype=np.float64)\n",
    "    keys = [f\"q{str(t).replace('.','_')}\" for t in taus]\n",
    "\n",
    "    Q = np.concatenate(\n",
    "        [np.asarray(preds_dict[k]).reshape(-1, 1).astype(np.float64) for k in keys],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    dQ = np.diff(Q, axis=1)\n",
    "    mono = np.all(dQ >= 0.0, axis=1)\n",
    "    crossing_rate = float(1.0 - np.mean(mono))\n",
    "    return crossing_rate\n",
    "\n",
    "\n",
    "def build_Xy_for_stations(station_ids, Y_used, phi_space_st, phi_time_t, Ds, D, T):\n",
    "    station_ids = np.asarray(station_ids, dtype=np.int64)\n",
    "    n_st = station_ids.shape[0]\n",
    "\n",
    "    X = np.empty((n_st * T, D), dtype=np.float32)\n",
    "    y = Y_used[station_ids].reshape(-1).astype(np.float32)\n",
    "\n",
    "    time_part = phi_time_t\n",
    "    for i, sid in enumerate(station_ids):\n",
    "        r0 = i * T\n",
    "        r1 = (i + 1) * T\n",
    "        X[r0:r1, :Ds] = phi_space_st[sid][None, :]\n",
    "        X[r0:r1, Ds:] = time_part\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "arr = np.load(DATA_PATH).astype(np.float32)\n",
    "S, V, T_full = arr.shape\n",
    "print(\"Weather2K shape:\", arr.shape)\n",
    "\n",
    "lat = arr[:, 0, 0]\n",
    "lon = arr[:, 1, 0]\n",
    "Y_full = arr[:, TARGET_IDX, :]\n",
    "\n",
    "if T_full < T_KEEP:\n",
    "    raise ValueError(f\"T_full={T_full} < T_KEEP={T_KEEP}\")\n",
    "\n",
    "Y = Y_full[:, -T_KEEP:]\n",
    "T = Y.shape[1]\n",
    "print(f\"Using last {T_KEEP} timesteps | Y shape: {Y.shape}\")\n",
    "\n",
    "lat_n = (lat - lat.min()) / (lat.max() - lat.min() + 1e-12)\n",
    "lon_n = (lon - lon.min()) / (lon.max() - lon.min() + 1e-12)\n",
    "s_xy = np.column_stack([lat_n, lon_n]).astype(np.float32)\n",
    "\n",
    "t_idx = np.arange(T, dtype=np.float32)\n",
    "t_norm = (t_idx - t_idx.min()) / (t_idx.max() - t_idx.min() + 1e-12)\n",
    "\n",
    "print(\"\\n[Stage 3] Precompute bases ...\")\n",
    "phi_space_st_full = build_space_basis(s_xy)\n",
    "phi_time_t_full = build_time_basis(t_norm)\n",
    "\n",
    "space_keep = (phi_space_st_full != 0).any(axis=0)\n",
    "time_keep = (phi_time_t_full != 0).any(axis=0)\n",
    "\n",
    "phi_space_st = phi_space_st_full[:, space_keep].astype(np.float32)\n",
    "phi_time_t = phi_time_t_full[:, time_keep].astype(np.float32)\n",
    "\n",
    "Ds = phi_space_st.shape[1]\n",
    "Dt = phi_time_t.shape[1]\n",
    "D = Ds + Dt\n",
    "\n",
    "print(f\"Space basis dim: {Ds}\")\n",
    "print(f\"Time  basis dim: {Dt}\")\n",
    "print(f\"Total embedding dim: {D}\")\n",
    "\n",
    "\n",
    "cross_list = []\n",
    "crps_z_list = []\n",
    "crps_raw_list = []\n",
    "\n",
    "for rep in range(1, N_REP + 1):\n",
    "    rep_seed = BASE_SEED + SEED_OFFSET + rep\n",
    "\n",
    "    random.seed(rep_seed)\n",
    "    np.random.seed(rep_seed)\n",
    "    tf.random.set_seed(rep_seed)\n",
    "\n",
    "    split_rs = np.random.RandomState(rep_seed)\n",
    "    n_obs = int(np.round(OBS_RATIO * S))\n",
    "    obs_sites = np.sort(split_rs.choice(S, size=n_obs, replace=False))\n",
    "\n",
    "    is_obs_station = np.zeros(S, dtype=bool)\n",
    "    is_obs_station[obs_sites] = True\n",
    "    miss_sites = np.where(~is_obs_station)[0]\n",
    "\n",
    "    n_val = max(1, int(np.round(VAL_STATION_RATIO * n_obs)))\n",
    "    perm = split_rs.permutation(n_obs)\n",
    "    val_sites = np.sort(obs_sites[perm[:n_val]])\n",
    "    train_sites = np.sort(obs_sites[perm[n_val:]])\n",
    "\n",
    "    y_train_raw = Y[train_sites].reshape(-1).astype(np.float32)\n",
    "    y_mu = float(np.mean(y_train_raw))\n",
    "    y_sd = float(np.std(y_train_raw) + 1e-12)\n",
    "    Yz = (Y - y_mu) / y_sd\n",
    "\n",
    "    X_train, y_train_z = build_Xy_for_stations(train_sites, Yz, phi_space_st, phi_time_t, Ds, D, T)\n",
    "    X_val, y_val_z = build_Xy_for_stations(val_sites, Yz, phi_space_st, phi_time_t, Ds, D, T)\n",
    "\n",
    "    y_train_dict = {f\"q{str(t).replace('.','_')}\": y_train_z for t in TAUS}\n",
    "    y_val_dict = {f\"q{str(t).replace('.','_')}\": y_val_z for t in TAUS}\n",
    "\n",
    "    n_chunks = int(np.ceil(len(miss_sites) / STATIONS_PER_CHUNK))\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    model = build_model_multi_quantile(D)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_dict,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val_dict),\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    sum_crps_z = 0.0\n",
    "    sum_crps_raw = 0.0\n",
    "    sum_cross = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for ci in range(n_chunks):\n",
    "        a = ci * STATIONS_PER_CHUNK\n",
    "        b = min((ci + 1) * STATIONS_PER_CHUNK, len(miss_sites))\n",
    "        chunk_sites = miss_sites[a:b]\n",
    "\n",
    "        X_chunk, y_true_z = build_Xy_for_stations(chunk_sites, Yz, phi_space_st, phi_time_t, Ds, D, T)\n",
    "        preds_z = model.predict(X_chunk, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "        crps_vec_z = crps_from_quantiles_weighted(y_true_z, preds_z, taus=TAUS)\n",
    "        sum_crps_z += float(np.sum(crps_vec_z))\n",
    "\n",
    "        y_true_raw = (y_true_z.astype(np.float64) * y_sd + y_mu)\n",
    "        preds_raw = {}\n",
    "        for tau in TAUS:\n",
    "            k = f\"q{str(tau).replace('.','_')}\"\n",
    "            preds_raw[k] = (np.asarray(preds_z[k]).reshape(-1).astype(np.float64) * y_sd + y_mu)\n",
    "\n",
    "        crps_vec_raw = crps_from_quantiles_weighted(y_true_raw, preds_raw, taus=TAUS)\n",
    "        sum_crps_raw += float(np.sum(crps_vec_raw))\n",
    "\n",
    "        cross_rate_chunk = crossing_rate_from_preds(preds_z, taus=TAUS)\n",
    "        sum_cross += float(cross_rate_chunk) * float(y_true_z.size)\n",
    "\n",
    "        count += int(y_true_z.size)\n",
    "\n",
    "        del X_chunk\n",
    "\n",
    "    crps_mean_z = sum_crps_z / count\n",
    "    crps_mean_raw = sum_crps_raw / count\n",
    "    crossing_rate = sum_cross / count\n",
    "\n",
    "    cross_list.append(crossing_rate)\n",
    "    crps_z_list.append(crps_mean_z)\n",
    "    crps_raw_list.append(crps_mean_raw)\n",
    "\n",
    "    print(\n",
    "        f\"[Rep {rep:02d}/{N_REP}] \"\n",
    "        f\"obs={len(obs_sites)} train={len(train_sites)} val={len(val_sites)} miss={len(miss_sites)} | \"\n",
    "        f\"Crossing rate = {crossing_rate:.6f}\"\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    del X_train, X_val\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "cross_arr = np.asarray(cross_list, dtype=np.float64)\n",
    "crps_z_arr = np.asarray(crps_z_list, dtype=np.float64)\n",
    "crps_raw_arr = np.asarray(crps_raw_list, dtype=np.float64)\n",
    "\n",
    "print(\"\\n=== Summary over repetitions (resample split each rep) ===\")\n",
    "print(f\"Crossing rate mean = {float(np.mean(cross_arr)):.6f}\")\n",
    "print(f\"Crossing rate SD   = {float(np.std(cross_arr, ddof=1)):.6f}\" if N_REP > 1 else \"Crossing rate SD   = NA\")\n",
    "print(f\"CRPS_z   mean = {float(np.mean(crps_z_arr)):.6f} | SD = {float(np.std(crps_z_arr, ddof=1)):.6f}\" if N_REP > 1 else f\"CRPS_z   mean = {float(np.mean(crps_z_arr)):.6f} | SD = NA\")\n",
    "print(f\"CRPS_raw mean = {float(np.mean(crps_raw_arr)):.6f} | SD = {float(np.std(crps_raw_arr, ddof=1)):.6f}\" if N_REP > 1 else f\"CRPS_raw mean = {float(np.mean(crps_raw_arr)):.6f} | SD = NA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepKriging.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
